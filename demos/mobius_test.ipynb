{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220aea83-7fd3-4969-acd2-727458ffa576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import TypeAlias\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.linalg as LA\n",
    "\n",
    "from transforms import MobiusMixtureTransform\n",
    "from utils import metropolis_acceptance, effective_sample_size\n",
    "from visualisations import scatter, pairplot\n",
    "\n",
    "Tensor: TypeAlias = torch.Tensor\n",
    "\n",
    "π = math.pi\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6b862-3089-4b7e-a2e2-aaa16420445d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CircularUniformPrior(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, sample_shape: int | list[int]):\n",
    "        super().__init__()\n",
    "        if not hasattr(sample_shape, \"__iter__\"):\n",
    "            sample_shape = [sample_shape]\n",
    "        if len(sample_shape) < 2:\n",
    "            sample_shape += [1]\n",
    "        self.shape = sample_shape\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        phi = torch.rand(self.shape) * 2 * π\n",
    "\n",
    "        x = phi.cos()\n",
    "        y = phi.sin()\n",
    "\n",
    "        outputs = torch.stack([x, y], dim=-1)\n",
    "        logq = (\n",
    "            torch.full(self.shape, fill_value=-math.log(2 * π), device=outputs.device)\n",
    "            .flatten(start_dim=1)\n",
    "            .sum(dim=1)\n",
    "        )\n",
    "\n",
    "        return outputs, logq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365f5ca-2bc0-4ba1-9ebf-f456c7718b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NormalizingFlowMobius(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        κ: float,\n",
    "        ξ: float,\n",
    "        n_layers: int,\n",
    "        n_mixture: int,\n",
    "        weighted: bool,\n",
    "        batch_size: int,\n",
    "        val_batch_size: int,\n",
    "        epsilon: float = 1e-3,\n",
    "        init_lr: float = 0.001,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.κ = κ\n",
    "        self.ξ = ξ\n",
    "        μ = torch.Tensor([math.cos(ξ), math.sin(ξ)])\n",
    "        self.register_buffer(\"μ\", μ)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.init_lr = init_lr\n",
    "\n",
    "        self.log_norm = -math.log(2 * π * torch.i0(torch.tensor([κ])))\n",
    "\n",
    "        self.transform = MobiusMixtureTransform(n_mixture, weighted, epsilon)\n",
    "        self.params = torch.nn.Parameter(\n",
    "            torch.stack(\n",
    "                [\n",
    "                    torch.rand_like(self.transform.identity_params)\n",
    "                    for _ in range(n_layers)\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Random global rotations applied after each layer (potentially trainable)\n",
    "        self.rotations = torch.nn.Parameter(torch.empty(n_layers).uniform_(0, 2 * π))\n",
    "        self.save_hyperparameters()\n",
    "        if self.logger is not None and type(self.logger) is TensorBoardLogger:\n",
    "            self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "    def forward(self, xy: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        ldj = torch.zeros(xy.shape[0], device=xy.device)\n",
    "\n",
    "        for params, rotation in zip(self.params, self.rotations):\n",
    "            # [-1, 1]\n",
    "            params = params.expand(*xy.shape[:-1], -1)\n",
    "            xy, ldj_this = self.transform(xy, params)\n",
    "\n",
    "            costheta, sintheta = rotation.cos(), rotation.sin()\n",
    "            R = torch.tensor(\n",
    "                [[costheta, sintheta], [-sintheta, costheta]],\n",
    "                device=xy.device,\n",
    "            ).view(*[1 for _ in xy.shape[:-1]], 2, 2)\n",
    "            xy_new = (R * xy.unsqueeze(-2)).sum(dim=-1)\n",
    "\n",
    "            x, y = xy.split(1, dim=-1)\n",
    "            xy = torch.cat(\n",
    "                [x * costheta + y * sintheta, x * -sintheta + y * costheta], dim=-1\n",
    "            )\n",
    "            assert torch.allclose(xy, xy_new)\n",
    "\n",
    "            ldj += ldj_this\n",
    "\n",
    "        xy.squeeze_(dim=1)\n",
    "\n",
    "        return xy, ldj\n",
    "\n",
    "    def training_step(self, batch, *_):\n",
    "        z, logq = batch\n",
    "        x, ldj = self(z)\n",
    "        logp = self.log_norm + self.κ * torch.mv(x, self.μ)\n",
    "\n",
    "        kl_div = torch.mean(logq - ldj - logp)\n",
    "        self.log(\"loss\", kl_div)\n",
    "\n",
    "        return kl_div\n",
    "\n",
    "    def validation_step(self, batch, *_):\n",
    "        z, logq = batch\n",
    "        x, ldj = self(z)\n",
    "        logp = self.log_norm + self.κ * torch.mv(x, self.μ)\n",
    "\n",
    "        weights = logp - (logq - ldj)\n",
    "        self.log(\"acceptance\", metropolis_acceptance(weights))\n",
    "        self.log(\"ess\", effective_sample_size(weights))\n",
    "\n",
    "        # Only TensorBoardLogger supported\n",
    "        if self.logger is None or type(self.logger) is not TensorBoardLogger:\n",
    "            return\n",
    "\n",
    "    def test_step(self, batch, *_):\n",
    "        z, logq = batch\n",
    "        x, ldj = self(z)\n",
    "        logp = self.log_norm + self.κ * torch.mv(x, self.μ)\n",
    "\n",
    "        weights = logp - (logq - ldj)\n",
    "        self.log(\"hp/acceptance\", metropolis_acceptance(weights))\n",
    "        self.log(\"hp/ess\", effective_sample_size(weights))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return CircularUniformPrior(self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return CircularUniformPrior(self.val_batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.val_dataloader()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.init_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8c8a2-e15f-4b52-aec8-8a9b287b76cd",
   "metadata": {},
   "source": [
    "## A single Mobius transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250805a-e239-4ba7-9b17-f43dbc1835c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NormalizingFlowMobius(\n",
    "    κ=10,\n",
    "    ξ=π / 2,\n",
    "    n_layers=1,\n",
    "    n_mixture=1,\n",
    "    weighted=False,\n",
    "    batch_size=2000,\n",
    "    val_batch_size=8000,\n",
    "    epsilon=1e-3,\n",
    "    init_lr=0.01,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_steps=2000,\n",
    "    val_check_interval=50,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=1,\n",
    "    num_sanity_val_steps=1,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "(metrics,) = trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d18c1a-1762-4571-9dd5-10aac1fa9ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xy_in, logq = next(CircularUniformPrior(1000))\n",
    "    xy, ldj = model(xy_in)\n",
    "\n",
    "r = LA.vector_norm(xy, dim=-1)\n",
    "x, y = xy.split(1, dim=-1)\n",
    "ϕ = torch.fmod(torch.atan2(y, x) + (2 * π), 2 * π)\n",
    "\n",
    "hist, bins = torch.histogram(ϕ, bins=36, range=(0, 2 * π), density=True)\n",
    "\n",
    "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10), subplot_kw={\"polar\": True})\n",
    "ax1.scatter(ϕ, r, edgecolor=\"black\", facecolor=\"yellow\", linewidths=0.5, alpha=0.5)\n",
    "ax1.set_ylim([0, 1.1])\n",
    "ax1.set_xticklabels([])\n",
    "\n",
    "ax2.bar(\n",
    "    0.5 * (bins[:-1] + bins[1:]), hist, width=(2 * π) / (len(bins) - 1), bottom=0.01\n",
    ")\n",
    "ax2.set_xticklabels([])\n",
    "\n",
    "\n",
    "fig2, ax3 = plt.subplots()\n",
    "ax3.set_xlabel(r\"$\\phi$\")\n",
    "ax3.set_ylabel(\"density\")\n",
    "ax3.scatter(ϕ, torch.exp(logq - ldj), s=0.4, label=\"model\")\n",
    "ax3.scatter(\n",
    "    ϕ,\n",
    "    torch.exp(model.log_norm + model.κ * torch.mv(xy, model.μ)),\n",
    "    s=0.4,\n",
    "    label=\"target\",\n",
    ")\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e9430-b065-466d-a925-eaac030031ba",
   "metadata": {},
   "source": [
    "## A mixture of Mobius transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342b2c8-007b-46cb-beba-1269f7cf3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NormalizingFlowMobius(\n",
    "    κ=10,\n",
    "    ξ=π / 2,\n",
    "    n_layers=1,\n",
    "    n_mixture=10,\n",
    "    weighted=True,\n",
    "    batch_size=2000,\n",
    "    val_batch_size=8000,\n",
    "    epsilon=1e-3,\n",
    "    init_lr=0.01,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_steps=2000,\n",
    "    val_check_interval=50,\n",
    "    limit_val_batches=1,\n",
    "    limit_test_batches=1,\n",
    "    num_sanity_val_steps=1,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "(metrics,) = trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37177e3-b92e-45f4-afa1-ea3d346e5dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xy_in, logq = next(CircularUniformPrior(1000))\n",
    "    xy, ldj = model(xy_in)\n",
    "\n",
    "r = LA.vector_norm(xy, dim=-1)\n",
    "x, y = xy.split(1, dim=-1)\n",
    "ϕ = torch.fmod(torch.atan2(y, x) + (2 * π), 2 * π)\n",
    "\n",
    "hist, bins = torch.histogram(ϕ, bins=36, range=(0, 2 * π), density=True)\n",
    "\n",
    "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10), subplot_kw={\"polar\": True})\n",
    "ax1.scatter(ϕ, r, edgecolor=\"black\", facecolor=\"yellow\", linewidths=0.5, alpha=0.5)\n",
    "ax1.set_ylim([0, 1.1])\n",
    "ax1.set_xticklabels([])\n",
    "\n",
    "ax2.bar(\n",
    "    0.5 * (bins[:-1] + bins[1:]), hist, width=(2 * π) / (len(bins) - 1), bottom=0.01\n",
    ")\n",
    "ax2.set_xticklabels([])\n",
    "\n",
    "fig2, ax3 = plt.subplots()\n",
    "ax3.set_xlabel(r\"$\\phi$\")\n",
    "ax3.set_ylabel(\"density\")\n",
    "ax3.scatter(ϕ, torch.exp(logq - ldj), s=0.4, label=\"model\")\n",
    "ax3.scatter(\n",
    "    ϕ,\n",
    "    torch.exp(model.log_norm + model.κ * torch.mv(xy, model.μ)),\n",
    "    s=0.4,\n",
    "    label=\"target\",\n",
    ")\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7224e-def5-46c9-9ece-137077c2c687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(2)\n",
    "x = x / x.pow(2).sum().sqrt()\n",
    "\n",
    "x1, x2 = x.split(1, dim=0)\n",
    "phi = torch.atan2(x2, x1)  # .remainder(2 * π)\n",
    "\n",
    "theta = torch.tensor([0])  # torch.rand(1)\n",
    "R = torch.tensor(\n",
    "    [\n",
    "        [theta.cos(), -theta.sin()],\n",
    "        [theta.sin(), theta.cos()],\n",
    "    ]\n",
    ")\n",
    "\n",
    "ya = torch.cat(\n",
    "    [\n",
    "        x1 * theta.cos() + x2 * -theta.sin(),\n",
    "        x1 * theta.sin() + x2 * theta.cos(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "yb = torch.mv(R, x)\n",
    "\n",
    "yc = (R * x.unsqueeze(0)).sum(dim=-1)\n",
    "\n",
    "phid = phi + theta  # .remainder(2 * π)\n",
    "yd = torch.cat([phid.cos(), phid.sin()])\n",
    "\n",
    "print(ya, yb, yc, yd)\n",
    "\n",
    "phia = torch.atan2(ya[1], ya[0])\n",
    "phib = torch.atan2(yb[1], yb[0])\n",
    "phic = torch.atan2(yc[1], yc[0])\n",
    "phidd = torch.atan2(yd[1], yd[0])\n",
    "\n",
    "print(phia, phib, phic, phid, phidd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c755f-b7d6-41ce-9d6c-cb94e94ce373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
