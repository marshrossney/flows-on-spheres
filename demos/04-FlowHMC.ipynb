{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3abc0b-8a16-4a87-84dd-66b816466338",
   "metadata": {},
   "source": [
    "# Flow HMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7eefa-4142-4e36-81e0-3f525cf6b93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import TypeAlias, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.linalg as LA\n",
    "\n",
    "from distributions import SphericalUniformPrior3D\n",
    "from models import RecursiveFlowS2, DummyNormalizingFlow\n",
    "from utils import batched_dot, batched_outer, batched_mv\n",
    "from visualisations import line, line3d, scatter, scatter3d, pairplot, spherical_mesh\n",
    "\n",
    "Tensor: TypeAlias = torch.Tensor\n",
    "\n",
    "π = math.pi\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bcf534-e0ef-4b1f-ba02-022bcab2352b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def debug_plots(\n",
    "    z: list,\n",
    "    x: list,\n",
    "    p: list,\n",
    "    F: list,\n",
    "    H: list,\n",
    "    S: list,\n",
    "    ldj: list,\n",
    "    *,\n",
    "    n_traj: int,\n",
    "    n_steps: int,\n",
    "    ε: float,\n",
    "    κ: float,\n",
    "    μ: Tensor,\n",
    "    lines: bool,\n",
    "):\n",
    "    z = torch.stack(z, dim=1)\n",
    "    x = torch.stack(x, dim=1)\n",
    "    p = torch.stack(p, dim=1)\n",
    "    F = torch.stack(F, dim=1)\n",
    "    H = torch.stack(H, dim=1)\n",
    "    S = torch.stack(S, dim=1)\n",
    "    ldj = torch.stack(ldj, dim=1)\n",
    "\n",
    "    modz = LA.vector_norm(z, dim=-1)\n",
    "    modx = LA.vector_norm(x, dim=-1)\n",
    "    modp = LA.vector_norm(p, dim=-1)\n",
    "    modF = LA.vector_norm(F, dim=-1)\n",
    "\n",
    "    phi_z = torch.fmod(torch.atan2(z[..., 1], z[..., 0]) + 2 * π, 2 * π)\n",
    "    phi_x = torch.fmod(torch.atan2(x[..., 1], x[..., 0]) + 2 * π, 2 * π)\n",
    "\n",
    "    pdotz = batched_dot(p, z)\n",
    "    Fdotz = batched_dot(F, z)\n",
    "\n",
    "    grad_ldj = F - κ * μ\n",
    "\n",
    "    nt = z.shape[1] - 1\n",
    "    t = np.linspace(0, nt * ε, nt + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        8, 1, sharex=True, figsize=(16, 24), gridspec_kw=dict(hspace=0.1)\n",
    "    )\n",
    "    for ax in axes:\n",
    "        [\n",
    "            ax.axvline(traj_num * n_steps * ε, linestyle=\"--\", color=\"grey\", alpha=0.5)\n",
    "            for traj_num in range(1, n_traj)\n",
    "        ]\n",
    "\n",
    "    if lines:\n",
    "        ls, ls2 = \"--\", \":\"\n",
    "        m, m2 = \"\", \"\"\n",
    "    else:\n",
    "        ls, ls2 = \"\", \"\"\n",
    "        m, m2 = \"+\", \"x\"\n",
    "\n",
    "    axes = iter(axes)\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(\"$z$\")\n",
    "    for i, zi in zip(range(1, 4), z.split(1, dim=-1)):\n",
    "        ax.plot(t, zi.squeeze(), marker=m, linestyle=ls, label=f\"$z_{i}$\")\n",
    "    ax.plot(t, modz.squeeze(), marker=m, linestyle=ls, label=r\"$|\\mathbf{z}|$\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(\"$x$\")\n",
    "    for i, xi in zip(range(1, 4), x.split(1, dim=-1)):\n",
    "        ax.plot(t, xi.squeeze(), marker=m, linestyle=ls, label=f\"$x_{i}$\")\n",
    "    ax.plot(t, modx.squeeze(), marker=m, linestyle=ls, label=r\"$|\\mathbf{x}|$\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(r\"$\\phi_z \\mid \\phi_x$\")\n",
    "    ax.plot(t, phi_z.squeeze(), marker=m, linestyle=ls, label=r\"$\\phi_z$\")\n",
    "    ax.plot(t, phi_x.squeeze(), marker=m2, linestyle=ls, label=r\"$\\phi_x$\")\n",
    "    ax.set_ylim(-0.5, 2 * π + 0.5)\n",
    "    ax.axhline(2 * π, linestyle=\":\")\n",
    "    ax.annotate(r\"$2 \\pi$\", xy=(0, 2 * π), xycoords=\"data\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(\"$p$\")\n",
    "    for i, pi in zip(range(1, 4), p.split(1, dim=-1)):\n",
    "        ax.plot(t, pi.squeeze(), marker=m, linestyle=ls, label=f\"$p_{i}$\")\n",
    "    ax.plot(t, modp.squeeze(), marker=m, linestyle=ls, label=r\"$|\\mathbf{p}|$\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(\"$F$\")\n",
    "    for i, Fi in zip(range(1, 4), F.split(1, dim=-1)):\n",
    "        ax.plot(t, Fi.squeeze(), marker=m, linestyle=ls, label=f\"$F_{i}$\")\n",
    "    ax.plot(t, modF.squeeze(), marker=m, linestyle=ls, label=r\"$|\\mathbf{F}|$\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(r\"$\\log \\vert \\partial \\mathbf{x} / \\partial \\mathbf{z} \\vert$\")\n",
    "    label = r\"$\\log \\vert \\partial \\mathbf{x} / \\partial \\mathbf{z} \\vert$\"\n",
    "    for i, gradi in zip(range(1, 4), grad_ldj.split(1, dim=-1)):\n",
    "        ax.plot(\n",
    "            t,\n",
    "            gradi.squeeze(),\n",
    "            marker=m2,\n",
    "            linestyle=ls,\n",
    "            label=f\"$\\partial / \\partial z_{i}$\" + label,\n",
    "        )\n",
    "    ax.plot(t, ldj.squeeze(), marker=m, linestyle=ls, label=label)\n",
    "    ax.legend()\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(\"$S$\")\n",
    "    ax.plot(t, (S + ldj).squeeze(), marker=m, linestyle=ls, label=\"$S_{vMF}$\")\n",
    "    ax.plot(\n",
    "        t,\n",
    "        (-ldj).squeeze(),\n",
    "        marker=m,\n",
    "        linestyle=ls,\n",
    "        label=r\"$-\\log \\vert \\partial \\mathbf{x} / \\partial \\mathbf{z} \\vert$\",\n",
    "    )\n",
    "    ax.plot(t, S.squeeze(), marker=m, linestyle=ls, label=r\"$\\tilde{S}$\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax = next(axes)\n",
    "    ax.set_ylabel(\"$H$\")\n",
    "    ax.plot(t, H.squeeze(), marker=m, linestyle=ls, label=\"$H$\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig2 = line(z.squeeze(), ls=\"\", marker=\".\", markersize=1)\n",
    "    fig3 = line3d(z.squeeze(), ls=\"-\", lw=0.7)\n",
    "\n",
    "    return fig, fig2, fig3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9fe5ea-6946-4e5b-a462-73d66927247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_forces(model: RecursiveFlowS2, inputs: Tensor, projection=\"aitoff\"):\n",
    "    \"\"\"\n",
    "    projection: aitoff, hammer, lambert or mollweide\n",
    "    \"\"\"\n",
    "\n",
    "    θ = torch.acos(1 - 2 * torch.linspace(0, 1, bins))\n",
    "    ϕ = torch.linspace(0, 2 * π, bins)\n",
    "    θ, ϕ = torch.meshgrid(θ, ϕ)\n",
    "    x = θ.sin() * ϕ.cos()\n",
    "    y = θ.sin() * ϕ.sin()\n",
    "    z = θ.cos()\n",
    "    inputs = torch.stack([x, y, z], dim=-1).view(-1, 3)\n",
    "\n",
    "    inputs.requires_grad_(True)\n",
    "    inputs.grad = None\n",
    "    with torch.enable_grad():\n",
    "        outputs, log_dxdz = model(inputs)\n",
    "        S_vMF = -model.κ * torch.mv(outputs, model.μ)\n",
    "        S_eff = S_vMF - log_dxdz\n",
    "        S_eff.backward(gradient=torch.ones_like(S_eff))\n",
    "    F = inputs.grad.negative()\n",
    "    inputs.requires_grad_(False)\n",
    "    inputs.grad = None\n",
    "\n",
    "    outputs = outputs.view(bins, bins, 3).detach()\n",
    "    F = F.view(bins, bins, 3).detach()\n",
    "    S_vMF = S_vMF.view(bins, bins).detach()\n",
    "    S_eff = S_eff.view(bins, bins).detach()\n",
    "\n",
    "    Fx, Fy, Fz = [Fi.squeeze() for Fi in F.split(1, dim=-1)]\n",
    "    mod_F = LA.vector_norm(F, dim=-1)\n",
    "\n",
    "    def make_heatmap(data, ax, title):\n",
    "        cf = ax.pcolormesh(\n",
    "            ϕ - π,\n",
    "            θ - π / 2,\n",
    "            data,\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "        fig.colorbar(cf, ax=ax, shrink=1)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    def symlog(x: Tensor):\n",
    "        y = torch.empty_like(x)\n",
    "        id_mask = x.abs() < 1\n",
    "        y[id_mask] = x[id_mask]\n",
    "        y[~id_mask] = (x[~id_mask].abs().log10() + 1) * x[~id_mask].sign()\n",
    "        return y\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        5, 2, figsize=(12, 12), subplot_kw=dict(projection=projection)\n",
    "    )\n",
    "    axes = iter(axes.flatten())\n",
    "\n",
    "    make_heatmap(S_vMF, next(axes), r\"$S_{vMF}$\")\n",
    "    make_heatmap(S_eff, next(axes), r\"$S_{vMF} - \\log_e \\mathcal{V}$\")\n",
    "    make_heatmap(Fx, next(axes), r\"$F_x$\")\n",
    "    make_heatmap(Fx.abs().log10(), next(axes), r\"$\\log_{10}|F_x|$\")\n",
    "    # make_heatmap(symlog(Fx), next(axes), r\"$\\log_{10}|F_x|$\")\n",
    "    make_heatmap(Fy, next(axes), r\"$F_y$\")\n",
    "    make_heatmap(Fy.abs().log10(), next(axes), r\"$\\log_{10}|F_y|$\")\n",
    "    # make_heatmap(symlog(Fy), next(axes), r\"$\\log_{10}|F_y|$\")\n",
    "    make_heatmap(Fz, next(axes), r\"$F_z$\")\n",
    "    make_heatmap(Fz.abs().log10(), next(axes), r\"$\\log_{10}|F_z|$\")\n",
    "    # make_heatmap(symlog(Fz), next(axes), r\"$\\log_{10}|F_z|$\")\n",
    "    make_heatmap(mod_F, next(axes), r\"$|\\mathbf{F}|$\")\n",
    "    make_heatmap(mod_F.log10(), next(axes), r\"$\\log_{10}|\\mathbf{F}|$\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Ideally I would use flowed coordinates for visualisations,\n",
    "    # but this causes issues with the heatmap\n",
    "    fig2 = scatter(\n",
    "        outputs.view(-1, 3),\n",
    "        colours=S_vMF.view(-1),\n",
    "        projection=projection,\n",
    "        s=0.1,\n",
    "        marker=\"x\",\n",
    "    )\n",
    "\n",
    "    return fig, fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292c2b8-cab9-4827-b8f3-ede9b8221bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualise_forces(model: RecursiveFlowS2, bins: int = 50, projection=\"aitoff\"):\n",
    "    \"\"\"\n",
    "    projection: aitoff, hammer, lambert or mollweide\n",
    "    \"\"\"\n",
    "\n",
    "    θ = torch.acos(1 - 2 * torch.linspace(0, 1, bins))\n",
    "    ϕ = torch.linspace(0, 2 * π, bins)\n",
    "    θ, ϕ = torch.meshgrid(θ, ϕ)\n",
    "    x = θ.sin() * ϕ.cos()\n",
    "    y = θ.sin() * ϕ.sin()\n",
    "    z = θ.cos()\n",
    "    inputs = torch.stack([x, y, z], dim=-1).view(-1, 3)\n",
    "\n",
    "    inputs.requires_grad_(True)\n",
    "    inputs.grad = None\n",
    "    with torch.enable_grad():\n",
    "        outputs, log_dxdz = model(inputs)\n",
    "        S_vMF = -model.κ * torch.mv(outputs, model.μ)\n",
    "        S_eff = S_vMF - log_dxdz\n",
    "        S_eff.backward(gradient=torch.ones_like(S_eff))\n",
    "    F = inputs.grad.negative()\n",
    "    inputs.requires_grad_(False)\n",
    "    inputs.grad = None\n",
    "\n",
    "    outputs = outputs.view(bins, bins, 3).detach()\n",
    "    F = F.view(bins, bins, 3).detach()\n",
    "    S_vMF = S_vMF.view(bins, bins).detach()\n",
    "    S_eff = S_eff.view(bins, bins).detach()\n",
    "\n",
    "    Fx, Fy, Fz = [Fi.squeeze() for Fi in F.split(1, dim=-1)]\n",
    "    mod_F = LA.vector_norm(F, dim=-1)\n",
    "\n",
    "    def make_heatmap(data, ax, title):\n",
    "        cf = ax.pcolormesh(\n",
    "            ϕ - π,\n",
    "            θ - π / 2,\n",
    "            data,\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "        fig.colorbar(cf, ax=ax, shrink=1)\n",
    "        ax.set_title(title)\n",
    "\n",
    "    def symlog(x: Tensor):\n",
    "        y = torch.empty_like(x)\n",
    "        id_mask = x.abs() < 1\n",
    "        y[id_mask] = x[id_mask]\n",
    "        y[~id_mask] = (x[~id_mask].abs().log10() + 1) * x[~id_mask].sign()\n",
    "        return y\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        5, 2, figsize=(12, 12), subplot_kw=dict(projection=projection)\n",
    "    )\n",
    "    axes = iter(axes.flatten())\n",
    "\n",
    "    make_heatmap(S_vMF, next(axes), r\"$S_{vMF}$\")\n",
    "    make_heatmap(S_eff, next(axes), r\"$S_{vMF} - \\log_e \\mathcal{V}$\")\n",
    "    make_heatmap(Fx, next(axes), r\"$F_x$\")\n",
    "    make_heatmap(Fx.abs().log10(), next(axes), r\"$\\log_{10}|F_x|$\")\n",
    "    # make_heatmap(symlog(Fx), next(axes), r\"$\\log_{10}|F_x|$\")\n",
    "    make_heatmap(Fy, next(axes), r\"$F_y$\")\n",
    "    make_heatmap(Fy.abs().log10(), next(axes), r\"$\\log_{10}|F_y|$\")\n",
    "    # make_heatmap(symlog(Fy), next(axes), r\"$\\log_{10}|F_y|$\")\n",
    "    make_heatmap(Fz, next(axes), r\"$F_z$\")\n",
    "    make_heatmap(Fz.abs().log10(), next(axes), r\"$\\log_{10}|F_z|$\")\n",
    "    # make_heatmap(symlog(Fz), next(axes), r\"$\\log_{10}|F_z|$\")\n",
    "    make_heatmap(mod_F, next(axes), r\"$|\\mathbf{F}|$\")\n",
    "    make_heatmap(mod_F.log10(), next(axes), r\"$\\log_{10}|\\mathbf{F}|$\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Ideally I would use flowed coordinates for visualisations,\n",
    "    # but this causes issues with the heatmap\n",
    "    fig2 = scatter(\n",
    "        outputs.view(-1, 3),\n",
    "        colours=S_vMF.view(-1),\n",
    "        projection=projection,\n",
    "        s=0.1,\n",
    "        marker=\"x\",\n",
    "    )\n",
    "\n",
    "    return fig, fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1be53-6eef-48b5-99bd-019a58375d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def flow_hmc(\n",
    "    trained_model: pl.LightningModule,\n",
    "    batch_size: int,\n",
    "    n_traj: int,\n",
    "    traj_length: int,\n",
    "    n_steps: int,\n",
    "    κ: Optional[float] = None,\n",
    "    μ: Optional[Tensor] = None,\n",
    "    debug: bool = False,\n",
    "    lines: bool = True,\n",
    "):\n",
    "    if debug and batch_size > 1:\n",
    "        raise ValueError(\"debug only works with a batch size of one\")\n",
    "\n",
    "    flow = trained_model\n",
    "\n",
    "    κ = κ if κ is not None else trained_model.κ\n",
    "    if μ is not None:\n",
    "        μ = μ if isinstance(μ, torch.Tensor) else torch.tensor(μ, dtype=torch.float32)\n",
    "        μ.div_(LA.vector_norm(μ))\n",
    "    μ = μ if μ is not None else trained_model.μ\n",
    "\n",
    "    ε = traj_length / n_steps\n",
    "\n",
    "    # Initial state randomly distributed on the sphere\n",
    "    z0, _ = next(SphericalUniformPrior3D(batch_size))\n",
    "    z0.squeeze_(dim=1)\n",
    "    assert z0.shape == torch.Size([batch_size, 3])\n",
    "\n",
    "    outputs = torch.empty(n_traj, batch_size, 3)\n",
    "\n",
    "    def get_force(z: Tensor) -> Tensor:\n",
    "        z.requires_grad_(True)\n",
    "        z.grad = None\n",
    "        with torch.enable_grad():\n",
    "            x, log_dxdz = flow(z)\n",
    "            S = -κ * torch.mv(x, μ) - log_dxdz\n",
    "            assert S.shape == torch.Size([batch_size])\n",
    "            S.backward(gradient=torch.ones_like(S))\n",
    "        F = z.grad.negative()\n",
    "        z.requires_grad_(False)\n",
    "        z.grad = None\n",
    "        return F\n",
    "\n",
    "    n_accepted = 0\n",
    "\n",
    "    # Quantities to track for debugging\n",
    "    H_history = []\n",
    "    F_history = []\n",
    "    z_history = []\n",
    "    p_history = []\n",
    "    x_history = []\n",
    "    S_history = []\n",
    "    ldj_history = []\n",
    "\n",
    "    for i in range(n_traj):\n",
    "        z = z0.clone()\n",
    "\n",
    "        H_history_i = []\n",
    "        F_history_i = []\n",
    "        z_history_i = []\n",
    "        p_history_i = []\n",
    "        x_history_i = []\n",
    "        S_history_i = []\n",
    "        ldj_history_i = []\n",
    "\n",
    "        # Initial momenta\n",
    "        p = torch.empty_like(z).normal_()\n",
    "        M = torch.eye(3) - batched_outer(z, z)\n",
    "        p = batched_mv(M, p)\n",
    "\n",
    "        # assert torch.allclose(batched_mv(M, z), torch.zeros(batch_size), atol=1e-5)\n",
    "        # assert torch.allclose(batched_dot(p, z), torch.zeros(batch_size), atol=1e-5)\n",
    "\n",
    "        x, log_dxdz = flow(z)\n",
    "        H0 = 0.5 * (p**2).sum(dim=1) - κ * torch.mv(x, μ) - log_dxdz\n",
    "\n",
    "        # Begin leapfrog\n",
    "\n",
    "        F = get_force(z)\n",
    "        dpdt = batched_mv(M, F)\n",
    "        p += 0.5 * ε * dpdt\n",
    "\n",
    "        # print(\"(PRE) |z| = \", LA.vector_norm(z, dim=-1))\n",
    "        # print(\"(PRE) z . π = \", batched_dot(π, z))\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            if debug:\n",
    "                x, log_dxdz = flow(z)\n",
    "                S = -κ * torch.mv(x, μ) - log_dxdz\n",
    "                pp = p - 0.5 * ε * dpdt  # move back half a step\n",
    "                H = 0.5 * (pp**2).sum(dim=1) + S\n",
    "                H_history_i.append(H)\n",
    "                F_history_i.append(F)\n",
    "                z_history_i.append(z)\n",
    "                p_history_i.append(pp)\n",
    "                x_history_i.append(x)\n",
    "                S_history_i.append(S)\n",
    "                ldj_history_i.append(log_dxdz)\n",
    "\n",
    "            # Non-trivial coordinate update to preserve unit norm\n",
    "            mod_p = LA.vector_norm(p, dim=-1, keepdim=True)\n",
    "            cos_εp = torch.cos(ε * mod_p)\n",
    "            sin_εp = torch.sin(ε * mod_p)\n",
    "            z_tmp = cos_εp * z + (1 / mod_p) * sin_εp * p\n",
    "            p = -mod_p * sin_εp * z + cos_εp * p\n",
    "            z = z_tmp\n",
    "\n",
    "            # print(\"|z| = \", LA.vector_norm(z, dim=-1))\n",
    "            # print(\"z . p = \", batched_dot(p, z))\n",
    "\n",
    "            # Re-normalise (correct for numerical errors)\n",
    "            z = z / LA.vector_norm(z, dim=-1, keepdim=True)\n",
    "\n",
    "            F = get_force(z)\n",
    "            M = torch.eye(3) - batched_outer(z, z)\n",
    "            dpdt = batched_mv(M, F)\n",
    "            if t < n_steps - 1:\n",
    "                p += ε * dpdt\n",
    "            elif t == n_steps - 1:\n",
    "                p += 0.5 * ε * dpdt\n",
    "            else:\n",
    "                raise Exception(\"whoops\")\n",
    "\n",
    "        x, log_dxdz = flow(z)\n",
    "        HT = 0.5 * (p**2).sum(dim=1) - κ * torch.mv(x, μ) - log_dxdz\n",
    "\n",
    "        accepted = (H0 - HT).clamp(max=0).exp() > torch.rand_like(H0)\n",
    "        n_accepted += accepted.sum()\n",
    "\n",
    "        z0[accepted] = z[accepted]\n",
    "\n",
    "        if debug and n_accepted > 0:  # only track history if trajectory accepted\n",
    "            # n.b. n_accepted = 0 or 1 since batch size must be 1 for debug\n",
    "            H_history += H_history_i\n",
    "            F_history += F_history_i\n",
    "            z_history += z_history_i\n",
    "            p_history += p_history_i\n",
    "            x_history += x_history_i\n",
    "            S_history += S_history_i\n",
    "            ldj_history += ldj_history_i\n",
    "\n",
    "        x, _ = flow(z0)\n",
    "        outputs[i] = x\n",
    "\n",
    "    acceptance_rate = n_accepted / (batch_size * n_traj)\n",
    "    print(\"acceptance: \", acceptance_rate)\n",
    "\n",
    "    if debug:\n",
    "        _ = debug_plots(\n",
    "            z_history,\n",
    "            x_history,\n",
    "            p_history,\n",
    "            F_history,\n",
    "            H_history,\n",
    "            S_history,\n",
    "            ldj_history,\n",
    "            n_traj=n_traj,\n",
    "            n_steps=n_steps,\n",
    "            ε=ε,\n",
    "            κ=κ,\n",
    "            μ=μ,\n",
    "            lines=lines,\n",
    "        )\n",
    "\n",
    "    return outputs.transpose(0, 1)  # (batch_size, n_traj, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6ab73-4054-4ac4-a1b6-2d756e5e8791",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the algorithm with a dummy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca9769-a014-4ee2-aca3-c901afc51da2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Uniform target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f3111-ace2-4e13-9d62-3299210194ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_model = DummyNormalizingFlow()\n",
    "\n",
    "x_from_flow_hmc = flow_hmc(\n",
    "    dummy_model,\n",
    "    batch_size=1,\n",
    "    n_traj=10,\n",
    "    traj_length=10,\n",
    "    n_steps=400,\n",
    "    κ=0.001,\n",
    "    μ=[0, 0, 1],\n",
    "    debug=True,\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b34fc5-8529-4a16-86b1-96572a0a7e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z, _ = next(SphericalUniformPrior3D(5000))\n",
    "    z.squeeze_(dim=1)\n",
    "    x_from_model, _ = dummy_model(z)\n",
    "\n",
    "x_from_flow_hmc = flow_hmc(\n",
    "    dummy_model,\n",
    "    batch_size=10,\n",
    "    n_traj=500,\n",
    "    traj_length=1,\n",
    "    n_steps=4,\n",
    "    κ=0.001,\n",
    "    μ=[0, 0, 1],\n",
    "    debug=False,\n",
    ")\n",
    "# x_from_flow_hmc = x_from_flow_hmc.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "fig1 = scatter(x_from_model, s=2)\n",
    "fig1.suptitle(\"Data Generated by Normalizing Flow\")\n",
    "\n",
    "fig2 = scatter(x_from_flow_hmc, s=2)\n",
    "fig2.suptitle(\"Data Generated by Flow HMC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ab2de-20b5-48bb-90f3-68d0f3b5915c",
   "metadata": {},
   "source": [
    "### Concentrated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73fda5-8386-4bab-8a76-a49f700499f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_model = DummyNormalizingFlow()\n",
    "\n",
    "x_from_flow_hmc = flow_hmc(\n",
    "    dummy_model,\n",
    "    batch_size=1,\n",
    "    n_traj=10,\n",
    "    traj_length=10,\n",
    "    n_steps=500,\n",
    "    κ=10,\n",
    "    μ=[1, -1, 1],\n",
    "    debug=True,\n",
    "    lines=True,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4288db2-e393-4791-adbd-ce9f14c7d407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_from_flow_hmc = flow_hmc(\n",
    "    dummy_model,\n",
    "    batch_size=10,\n",
    "    n_traj=500,\n",
    "    traj_length=1,\n",
    "    n_steps=4,\n",
    "    κ=10,\n",
    "    μ=[1, -1, 1],\n",
    "    debug=False,\n",
    ")\n",
    "# x_from_flow_hmc = x_from_flow_hmc.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "fig1 = scatter(x_from_model, s=2)\n",
    "fig1.suptitle(\"Data Generated by Normalizing Flow\")\n",
    "\n",
    "fig2 = scatter(x_from_flow_hmc, s=2)\n",
    "fig2.suptitle(\"Data Generated by Flow HMC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc453b3-1115-4543-ba45-67e201f48819",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d7968-45a1-42fa-b819-999ead84c8f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"tb_logs/test/version_6/checkpoints/last.ckpt\"\n",
    "\n",
    "model = RecursiveFlowS2.load_from_checkpoint(ckpt_path)\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f39e0b-c04b-4cdf-8bc0-fb5fb6e06be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(limit_test_batches=1, logger=False)\n",
    "\n",
    "(metrics,) = trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5518071-355c-4d20-9099-506b25e3853d",
   "metadata": {},
   "source": [
    "## Visualise the forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafd5d0-ab53-4cee-ae2c-ff76ca4a728f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = forces_heatmap(model, bins=100, projection=\"lambert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d4ae4-44a8-4fe2-bb7d-cf6603ea8ff7",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e5a24-728b-4e9d-82b9-378a7fb96edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z, _ = next(SphericalUniformPrior3D(5000))\n",
    "    z.squeeze_(dim=1)\n",
    "    x_from_model, _ = model(z)\n",
    "\n",
    "x_from_flow_hmc = flow_hmc(\n",
    "    model, batch_size=10, n_traj=500, traj_length=1, n_steps=4, debug=False\n",
    ")\n",
    "\n",
    "fig1 = scatter(x_from_model, s=2)\n",
    "fig1.suptitle(\"Data Generated by Normalizing Flow\")\n",
    "\n",
    "fig2 = scatter(x_from_flow_hmc, s=2)\n",
    "fig2.suptitle(\"Data Generated by Flow HMC\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb7cac-2162-4136-97d5-67a1ba3ff5d7",
   "metadata": {},
   "source": [
    "## Visualise trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa649eb7-98e0-4aff-8639-f592d1d1fa40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_from_flow_hmc = flow_hmc(\n",
    "    model, batch_size=1, n_traj=4, traj_length=4, n_steps=400, debug=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
